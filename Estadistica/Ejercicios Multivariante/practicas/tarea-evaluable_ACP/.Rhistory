data[is.na(data)]<-mean(data,na.rm=T)
}
data
}
datos_pca.default
datos_pca:defaul
datos_pca
library(foreign)
datos<-read.spss("empresas.sav", to.data.frame = TRUE, reencode="utf-8")
round(datos,4) # Pone que hay 13 empresas y hay 14 filas
# La fila 14 tiene valores extraños (20023 en X1) y datos perdidos, así que la eliminamos
summary(datos) # Observamos que summary indica si hay datos perdidos
datos<-datos[-14,]
datos
summary(datos) # Análisis de la escala y variabilidad de cada variable, también vemos que no hay ningún dato perdido
boxplot(datos,main="Análisis exploratorio de datos",
xlab="Indicadores económicos",
ylab="Distribución de valores",
col=c(1:15))
# Compruebo si los datos están estandarizados (media 0 y varianza 1)
round(colMeans(datos),4)
round(apply(datos, 2, sd),4)
# No están estandarizados, lo hacemos nosotros
datos_pca<-scale(datos)
datos_pca
round(colMeans(datos_pca),4)
round(apply(datos_pca, 2, sd),4)
round(cor(datos_pca),4) # Matriz de correlación
# Test de Bartlett
install.packages("psych")
library(psych)
cortest.bartlett(cor(datos_pca),n=13)
datos_originales<-datos_pca # Guardamos los datos para comparar con los valores perdidos
# Función para limpiar los outliers
outlier<-function(data){
continue<-TRUE
while(continue){
H<-1.5*IQR(data)
data[data<quantile(data,0.25,na.rm = T)-H]<-NA
data[data>quantile(data,0.75, na.rm = T)+H]<-NA
continue<-any(is.na(data))
data[is.na(data)]<-mean(data,na.rm=T)
}
data
}
datos_pca
datos_pca[0]
View(datos_pca)
datos_pca[,4]
datos_pca[,4]
# La aplicamos a cada una de las variables que presenta outliers
datos_pca[,4]<-outlier(datos_pca[,4])
datos_pca[,5]<-outlier(datos_pca[,5])
# Comparamos los datos originales y los arreglados
par(mfrow=c(1,2))
# Boxplot de los datos originales
boxplot(datos_originales,main="Datos originales",
xlab="Indicadores económicos",
ylab="z-values",
col=c(1:15))
# Boxplot de los datos corregidos.
boxplot(datos_pca,main="Datos sin outliers",
xlab="Indicadores económicos",
ylab="z-values",
col=c(1:15))
# ACP con la función prcomp
PCA<-prcomp(datos_pca, scale=T, center = T)
# Matriz de cada rotación, con los pesos de cada una de las variables
# en cada una de las componentes principales.
PCA$rotation
# En el campo "sdev" del objeto "PCA" y con la funci?n summary aplicada
# al objeto, obtenemos informaci?n relevante: desviaciones t?picas de
# cada componente principal, proporci?n de varianza explicada y acumlada.
PCA$sdev
# Matriz de cada rotación, con los pesos de cada una de las variables
# en cada una de las componentes principales.
round(PCA$rotation,4)
# En el campo "sdev" del objeto "PCA" y con la funci?n summary aplicada
# al objeto, obtenemos informaci?n relevante: desviaciones t?picas de
# cada componente principal, proporci?n de varianza explicada y acumlada.
PCA$sdev
# Desviación típica de cada componente principal
PCA$sdev
summary(PCA)
# Proporción de la varianza explicada y acumulada de cada componente
summary(PCA)
install.packages("ggplot2")
library("ggplot2")
# ACP con la función prcomp
PCA<-prcomp(datos_pca, scale=T, center = T)
# Matriz de cada rotación, con los pesos de cada una de las variables
# en cada una de las componentes principales.
round(PCA$rotation,4)
# Proporción de la varianza explicada y acumulada de cada componente
summary(PCA)
PCA$sdev^2
mean(PCA$sdev^2)
round(mean(PCA$sdev^2),4)
round(PCA$sdev^2,4)
round(mean(PCA$sdev^2),4)
# Proporción de la varianza explicada y acumulada de cada componente
summary(PCA)
ggplot(data = data.frame(varianza_explicada, pc = 1:15),
aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporci?n de varianza explicada")
ggplot(data = data.frame(prop_var, pc = 1:15),
aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporci?n de varianza explicada")
# Proporción de varianza explicada
prop_var<- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(prop_var, pc = 1:15),
aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporci?n de varianza explicada")
# Proporción de varianza explicada
prop_var<- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(prop_var, pc = 1:15),
aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporci?n de varianza explicada")
ggplot(data = data.frame(prop_var, pc = 1:8),
aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporción de varianza explicada")
ggplot(data = data.frame(prop_var, pc = 1:8),
aes(x = pc, y = prop_var, fill=prop_var )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporción de varianza explicada")
ggplot( data = data.frame(cum_var, pc = 1:15),
aes(x = pc, y = cum_var ,fill=varianza_acum )) +
geom_col(width = 0.5) +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporción de varianza acumulada")
# Proporción acumulado de varianza explicada
cum_var<-cumsum(prop_var)
ggplot( data = data.frame(cum_var, pc = 1:8),
aes(x = pc, y = cum_var ,fill=varianza_acum )) +
geom_col(width = 0.5) +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporción de varianza acumulada")
ggplot( data = data.frame(cum_var, pc = 1:8),
aes(x = pc, y = cum_var ,fill=cum_var )) +
geom_col(width = 0.5) +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporción de varianza acumulada")
# Carga del paquete "archdata" si est? instalado
library("archdata")
# Carga y asignaci?n a una variable del conjunto de datos de inter?s "RBGlass1
data("RBGlass1")
datos<-RBGlass1
# La variable "datos" es un data.frame
# Eliminamos la primera columna del data.frame porque la ubicaci?n del resto
# de vidrio no aporta nada al ACP (un ?ndice negativo en cualquier objeto de
# R indica que esa dimensi?n es eliminada)
datos_pca<-datos[,-1]
# Guardo los datos originales porque la variable datos_pca va a cambiar a lo
# largo del an?lisis
datos_originales<-datos_pca
# La funci?n "head" muestra unos cuantos datos del data.frame aunque trabajmos
# con las 105 muestras de vidrio recogida
head(datos_pca)
# PASO 1: ?tiene sentido un ACP?
# --------------------------------------
# Para responder a esta pregunta se comprueba si existe correlaci?n entre las
# variables con la funci?n "cor" del paquete base, que proporciona la matriz
# de correlaciones R
cor(datos_pca)
# Observando la matriz de datos existe correlaci?n importante entre algunas
# variables, como sodio (NA) y antimonio (Sb) o titanio (Ti) e hierro (Fe)
cor(datos_pca$Na,datos_pca$Sb)
cor(datos_pca$Ti,datos_pca$Fe)
# El contraste de esfericidad de Bartlett permite comprobar si las correlaciones
# son distintas de 0 de modo significativo. La hip?tesis nula es que det(R)=1
# La funci?n "cortest.bartlett" del paquete "pysch" reliza este test.
# Esta funci?n trabaja con datos normalizados
# Instalaci?n del paquete desde un repositorio en caso de no estar instalado
install.packages("psych")
# Se normalizan los datos
datos_normalizados<-scale(datos_pca)
# Se hace el test de esfericidad
cortest.bartlett(cor(datos_normalizados))
# ------------------------------------------ #
# Paso 2: an?lisis exploratorio de los datos #
# ------------------------------------------ #
# El objetivo es el de localizar outliers que puedan dar lugar a resultados
# err?neos ya que el ACP es muy sensible a valores extremos. Un diagrama de
# cajas puede dar esta primera informaci?n.
boxplot(datos_pca,main="An?lisis exploratorio de datos",
xlab="Elementos qu?micos",
ylab="% de concentraci?n",
col=c(1:11))
# Los outliers deben ser tratados de forma independiente por el investigador,
# de modo que para el ACP es necesario eliminarlos
# La funci?n outlier definida como sigue elimina los outliers sustituy?ndolos
# por los promedios del resto de valores.
# Ejemplo de construcci?n de una funci?n en R
outlier<-function(data,na.rm=T){
H<-1.5*IQR(data)
data[data<quantile(data,0.25,na.rm = T)-H]<-NA
data[data>quantile(data,0.75, na.rm = T)+H]<-NA
data[is.na(data)]<-mean(data,na.rm=T)
data
}
# A continuaci?n aplicamos esta funci?n a cada uno a de las variables
datos_pca$Mg<-outlier(datos_pca$Mg)
datos_pca$Ca<-outlier(datos_pca$Ca)
datos_pca$K<-outlier(datos_pca$K)
datos_pca$P<-outlier(datos_pca$P)
datos_pca$Mn<-outlier(datos_pca$Mn)
# Comparamos los datos originales y los arreglados
# Esta funci?n divide la salida gr?fica en dos columnas
par(mfrow=c(1,2))
# Boxplot de los datos originales
boxplot(datos_originales,main="Datos originales",
xlab="Elementos qu?micos",
ylab="% de concentraci?n",
col=c(1:11))
# Boxplot de los datos corregidos.
boxplot(datos_pca,main="Datos sin outliers",
xlab="Elementos qu?micos",
ylab="% de concentraci?n",
col=c(1:11))
# La funci?n "prcomp" del paquete base de R realiza este an?lisis
# Pasamos los par?metros "scale" y "center" a TRUE para consideras
# los datos originales normalizados
PCA=prcomp(datos_pca, scale=T, center = T)
# El el campo "rotation" del objeto "PCA" es una matrix cuyas columnas
# son los coeficientes de las componentes principales, es decir, el
# peso de cada variable en la correspondiente componente principal
PCA$rotation
# En el campo "sdev" del objeto "PCA" y con la funci?n summary aplicada
# al objeto, obtenemos informaci?n relevante: desviaciones t?picas de
# cada componente principal, proporci?n de varianza explicada y acumlada.
PCA$sdev
summary(PCA)
# Instalaci?n del paquete desde un repositorio en caso de no estar instalado
install.packages("ggplot2")
# Carga del paquete "ggplot2" si est? instalado
library("ggplot2")
# El siguiente gr?fico muestra la proporci?n de varianza explicada
varianza_explicada <- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:11),
aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporci?n de varianza explicada")
# El siguiente gr?fico muestra la proporci?n de varianza explicada
varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:11),
aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
geom_col(width = 0.5) +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporci?n varianza acumulada")
# Paso 4: selecci?n del n?mero de componentes principales ?ptimo #
# -------------------------------------------------------------- #
# Existen diferentes m?todos:
# 1.- M?todo del codo (Cuadras, 2007). Ejercicio: buscar informaci?n (voluntario)
# 2.- A criterio del investigador que elige un porcentaje m?nmo de varianza explicada
# por las componentes principales (no es fiable porque puede dar m?s de las necesarias.
# 3.- En este caso se utiliza la regla de Abdi et al. (2010). Se promedia las varianzas
# explicadas por la componentes principales y se selecciona aquellas cuya proporci?n de
# varianza explicada supera la media.
# En este caso se eligen tan solo tres direcciones principales tal y como se puede ver
PCA$sdev^2
mean(PCA$sdev^2)
# Instalaci?n del paquete desde un repositorio en caso de no estar instalado
install.packages("factoextra")
ggplot( data = data.frame(cum_var, pc = 1:8),
aes(x = pc, y = cum_var ,fill=cum_var )) +
geom_line(size = 1.5) +
geom_point(size=3, fill="black")+
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporción de varianza acumulada")
ggplot( data = data.frame(cum_var, pc = 1:8),
aes(x = pc, y = cum_var ,fill=cum_var )) +
geom_line(size = 1.5) +
geom_point(size=3, fill="black") +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporción de varianza acumulada")
# Método del codo
ggplot( data = data.frame(cum_var, pc = 1:8),
aes(x = pc, y = cum_var ,fill=cum_var )) +
geom_line(size = 1.5) +
geom_point(size=3, fill="black") +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporción de varianza acumulada")
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,axes=c(1,2),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Carga del paquete "factorextra" si est? instalado
library("factoextra")
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,axes=c(1,2),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Esto produce una comparativa entre la primera y segunda componente principal analizando
# que variables tienen m?s peso para la definici?n de cada componente principal
fviz_pca_var(PCA,
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
library(foreign)
datos<-read.spss("empresas.sav", to.data.frame = TRUE, reencode="utf-8")
round(datos,4) # Pone que hay 13 empresas y hay 14 filas
# La fila 14 tiene valores extraños (20023 en X1) y datos perdidos, así que la eliminamos
summary(datos) # Observamos que summary indica si hay datos perdidos
datos<-datos[-14,]
datos
summary(datos) # Análisis de la escala y variabilidad de cada variable, también vemos que no hay ningún dato perdido
boxplot(datos,main="Análisis exploratorio de datos",
xlab="Indicadores económicos",
ylab="Distribución de valores",
col=c(1:15))
# Compruebo si los datos están estandarizados (media 0 y varianza 1)
round(colMeans(datos),4)
round(apply(datos, 2, sd),4)
# No están estandarizados, lo hacemos nosotros
datos_pca<-scale(datos)
datos_pca
round(colMeans(datos_pca),4)
round(apply(datos_pca, 2, sd),4)
round(cor(datos_pca),4) # Matriz de correlación
# Test de Bartlett
install.packages("psych")
library(psych)
cortest.bartlett(cor(datos_pca),n=13)
datos_originales<-datos_pca # Guardamos los datos para comparar con los valores perdidos
# Función para limpiar los outliers
outlier<-function(data){
continue<-TRUE
while(continue){
H<-1.5*IQR(data)
data[data<quantile(data,0.25,na.rm = T)-H]<-NA
data[data>quantile(data,0.75, na.rm = T)+H]<-NA
continue<-any(is.na(data))
data[is.na(data)]<-mean(data,na.rm=T)
}
data
}
# La aplicamos a cada una de las variables que presenta outliers
datos_pca[,4]<-outlier(datos_pca[,4])
datos_pca[,5]<-outlier(datos_pca[,5])
# Comparamos los datos originales y los arreglados
par(mfrow=c(1,2))
# Boxplot de los datos originales
boxplot(datos_originales,main="Datos originales",
xlab="Indicadores económicos",
ylab="z-values",
col=c(1:15))
# Boxplot de los datos corregidos.
boxplot(datos_pca,main="Datos sin outliers",
xlab="Indicadores económicos",
ylab="z-values",
col=c(1:15))
# ACP con la función prcomp
PCA<-prcomp(datos_pca, scale=T, center = T)
# Matriz de cada rotación, con los pesos de cada una de las variables
# en cada una de las componentes principales.
round(PCA$rotation,4)
# Desviación típica de cada componente principal
PCA$sdev
# Proporción de la varianza explicada y acumulada de cada componente
summary(PCA)
install.packages("ggplot2")
library("ggplot2")
# Proporción de varianza explicada
prop_var<- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(prop_var, pc = 1:8),
aes(x = pc, y = prop_var, fill=prop_var )) +
geom_col(width = 0.3) +
scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
labs(x = "Componente principal", y= " Proporción de varianza explicada")
# Proporción acumulada de varianza explicada
cum_var<-cumsum(prop_var)
ggplot( data = data.frame(cum_var, pc = 1:8),
aes(x = pc, y = cum_var ,fill=cum_var )) +
geom_col(width = 0.5) +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporción de varianza acumulada")
# Variables cuya varianza supera a la media de las varianzas
round(PCA$sdev^2,4)
round(mean(PCA$sdev^2),4)
# Método del codo
ggplot( data = data.frame(cum_var, pc = 1:8),
aes(x = pc, y = cum_var ,fill=cum_var )) +
geom_line(size = 1.5) +
geom_point(size=3, fill="black") +
scale_y_continuous(limits = c(0,1)) +
theme_bw() +
labs(x = "Componente principal",
y = "Proporción de varianza acumulada")
install.packages("factoextra")
library("factoextra")
# Esto produce una comparativa entre la primera y segunda componente principal analizando
# que variables tienen m?s peso para la definici?n de cada componente principal
fviz_pca_var(PCA,
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
# Esto produce una comparativa entre la primera y tercera componente principal analizando
# que variables tienen m?s peso para la definici?n de cada componente principal
fviz_pca_var(PCA,axes=c(1,3),
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
# Esto produce una comparativa entre la segunda y tercera componente principal analizando
# que variables tienen m?s peso para la definici?n de cada componente principal
fviz_pca_var(PCA,axes=c(2,3),
repel=TRUE,col.var="cos2",
legend.title="Distancia")+theme_bw()
# Observaciones en la primera y segunda componente principal
fviz_pca_ind(PCA,col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var")+theme_bw()
# Observaciones en la primera y tercera componente principal
fviz_pca_ind(PCA,axes=c(1,3),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var")+theme_bw()
# Observaciones en la segunda y tercera componente principal
fviz_pca_ind(PCA,axes=c(2,3),col.ind = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel=TRUE,legend.title="Contrib.var")+theme_bw()
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 1?  y 3? componente principal
fviz_pca(PCA,axes=c(1,3),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,axes=c(1,2),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA, axes=c(1,2),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 1?  y 3? componente principal
fviz_pca(PCA,axes=c(1,3),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,axes=c(2,3),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA, axes=c(1,2),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
fviz_pca(PCA,axes=c(2,3),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
fviz_pca(PCA, axes=c(1,2),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
fviz_pca(PCA,axes=c(1,3),
alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
repel=TRUE,
legend.title="Distancia")+theme_bw()
pca$x
PCA$x
PCA$x[,1:3]
round(PCA$x[,1:3],4)
transpose(round(PCA$x[,1:3],4))
t(round(PCA$x[,1:3],4))
t(round(PCA$x[,1:3],3))
